<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">












<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=6.4.2" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.4.2">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.4.2">


  <link rel="mask-icon" href="/images/logo.svg?v=6.4.2" color="#222">









<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '6.4.2',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Talk is cheap, show me the code!">
<meta property="og:type" content="website">
<meta property="og:title" content="张君腾的博客">
<meta property="og:url" content="https://www.juntengzhang.com/index.html">
<meta property="og:site_name" content="张君腾的博客">
<meta property="og:description" content="Talk is cheap, show me the code!">
<meta property="og:locale" content="zh-CN">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="张君腾的博客">
<meta name="twitter:description" content="Talk is cheap, show me the code!">






  <link rel="canonical" href="https://www.juntengzhang.com/">



<script type="text/javascript" id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>张君腾的博客</title>
  









  <noscript>
  <style type="text/css">
    .use-motion .motion-element,
    .use-motion .brand,
    .use-motion .menu-item,
    .sidebar-inner,
    .use-motion .post-block,
    .use-motion .pagination,
    .use-motion .comments,
    .use-motion .post-header,
    .use-motion .post-body,
    .use-motion .collection-title { opacity: initial; }

    .use-motion .logo,
    .use-motion .site-title,
    .use-motion .site-subtitle {
      opacity: initial;
      top: initial;
    }

    .use-motion {
      .logo-line-before i { left: initial; }
      .logo-line-after i { right: initial; }
    }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">张君腾的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">
    <a href="/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-about">
    <a href="/about/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-user"></i> <br>关于</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">
    <a href="/tags/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>
  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">
    <a href="/archives/" rel="section">
      <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>
  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2019/04/25/attention简介/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/25/attention简介/" itemprop="url">
                  attention简介
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2019-04-25 16:45:48 / 修改时间：17:18:43" itemprop="dateCreated datePublished" datetime="2019-04-25T16:45:48+08:00">2019-04-25</time>
            

            
              

              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/04/25/attention简介/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2019/04/25/attention简介/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>有关attention的源码在’lib/python3.5/site-packages/tensorflow/contrib/seq2seq’，一个attention解码器例子：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">def attention_decoder(inputs, memory, num_units=None, scope=&quot;attention_decoder&quot;, reuse=None):</span><br><span class="line">    &apos;&apos;&apos;Applies a GRU to `inputs`, while attending `memory`.</span><br><span class="line">    Args:</span><br><span class="line">      inputs: A 3d tensor with shape of [N, T&apos;, C&apos;]. Decoder inputs.</span><br><span class="line">      memory: A 3d tensor with shape of [N, T, C]. Outputs of encoder network.</span><br><span class="line">      num_units: An int. Attention size.</span><br><span class="line">      scope: Optional scope for `variable_scope`.</span><br><span class="line">      reuse: Boolean, whether to reuse the weights of a previous layer</span><br><span class="line">        by the same name.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      A 3d tensor with shape of [N, T, num_units].</span><br><span class="line">    &apos;&apos;&apos;</span><br><span class="line">    with tf.variable_scope(scope, reuse=reuse):</span><br><span class="line">        if num_units is None:</span><br><span class="line">            num_units = inputs.get_shape().as_list[-1]</span><br><span class="line">        #attention机制，根据memory和query生成alignm,ents</span><br><span class="line">        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(num_units,</span><br><span class="line">                                                                   memory)</span><br><span class="line">        #标准GRU单元，继承RNN</span><br><span class="line">        decoder_cell = tf.contrib.rnn.GRUCell(num_units)</span><br><span class="line">        #attention单元，继承RNN</span><br><span class="line">        cell_with_attention = tf.contrib.seq2seq.AttentionWrapper(decoder_cell,</span><br><span class="line">                                                                  attention_mechanism,</span><br><span class="line">                                                                  num_units,</span><br><span class="line">                                                                  alignment_history=True)</span><br><span class="line">        #动态rnn</span><br><span class="line">        outputs, state = tf.nn.dynamic_rnn(cell_with_attention, inputs, dtype=tf.float32) #( N, T&apos;, 16)</span><br><span class="line"></span><br><span class="line">    return outputs, state</span><br></pre></td></tr></table></figure></p>
<p>attention_mechanism根据memory和query生成alignments；decoder_cell为rnn单元；AttentionWrapper封装decoder_cell和attention_mechanism生成新的attention单元；下面详细讲解。</p>
<h2 id="attention-mechanism"><a href="#attention-mechanism" class="headerlink" title="attention_mechanism"></a>attention_mechanism</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line">def _bahdanau_score(processed_query, keys, normalize):</span><br><span class="line">  #根据query和memory(key)生成score，作为softmax的输入</span><br><span class="line">  &quot;&quot;&quot;Implements Bahdanau-style (additive) scoring function.</span><br><span class="line"></span><br><span class="line">  This attention has two forms.  The first is Bhandanau attention,</span><br><span class="line">  as described in:</span><br><span class="line"></span><br><span class="line">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span><br><span class="line">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span><br><span class="line">  ICLR 2015. https://arxiv.org/abs/1409.0473</span><br><span class="line"></span><br><span class="line">  The second is the normalized form.  This form is inspired by the</span><br><span class="line">  weight normalization article:</span><br><span class="line"></span><br><span class="line">  Tim Salimans, Diederik P. Kingma.</span><br><span class="line">  &quot;Weight Normalization: A Simple Reparameterization to Accelerate</span><br><span class="line">   Training of Deep Neural Networks.&quot;</span><br><span class="line">  https://arxiv.org/abs/1602.07868</span><br><span class="line"></span><br><span class="line">  To enable the second form, set `normalize=True`.</span><br><span class="line"></span><br><span class="line">  Args:</span><br><span class="line">    processed_query: Tensor, shape `[batch_size, num_units]` to compare to keys.</span><br><span class="line">    keys: Processed memory, shape `[batch_size, max_time, num_units]`.</span><br><span class="line">    normalize: Whether to normalize the score function.</span><br><span class="line"></span><br><span class="line">  Returns:</span><br><span class="line">    A `[batch_size, max_time]` tensor of unnormalized score values.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  dtype = processed_query.dtype</span><br><span class="line">  # Get the number of hidden units from the trailing dimension of keys</span><br><span class="line">  num_units = keys.shape[2].value or array_ops.shape(keys)[2]</span><br><span class="line">  # Reshape from [batch_size, ...] to [batch_size, 1, ...] for broadcasting.</span><br><span class="line">  processed_query = array_ops.expand_dims(processed_query, 1)</span><br><span class="line">  v = variable_scope.get_variable(</span><br><span class="line">      &quot;attention_v&quot;, [num_units], dtype=dtype)</span><br><span class="line">  if normalize:</span><br><span class="line">    # Scalar used in weight normalization</span><br><span class="line">    g = variable_scope.get_variable(</span><br><span class="line">        &quot;attention_g&quot;, dtype=dtype,</span><br><span class="line">        initializer=init_ops.constant_initializer(math.sqrt((1. / num_units))),</span><br><span class="line">        shape=())</span><br><span class="line">    # Bias added prior to the nonlinearity</span><br><span class="line">    b = variable_scope.get_variable(</span><br><span class="line">        &quot;attention_b&quot;, [num_units], dtype=dtype,</span><br><span class="line">        initializer=init_ops.zeros_initializer())</span><br><span class="line">    # normed_v = g * v / ||v||</span><br><span class="line">    normed_v = g * v * math_ops.rsqrt(</span><br><span class="line">        math_ops.reduce_sum(math_ops.square(v)))</span><br><span class="line">    return math_ops.reduce_sum(</span><br><span class="line">        normed_v * math_ops.tanh(keys + processed_query + b), [2])</span><br><span class="line">  else:</span><br><span class="line">    return math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query), [2])</span><br><span class="line"></span><br><span class="line">class BahdanauAttention(_BaseAttentionMechanism):</span><br><span class="line">  &quot;&quot;&quot;Implements Bahdanau-style (additive) attention.</span><br><span class="line"></span><br><span class="line">  This attention has two forms.  The first is Bahdanau attention,</span><br><span class="line">  as described in:</span><br><span class="line"></span><br><span class="line">  Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio.</span><br><span class="line">  &quot;Neural Machine Translation by Jointly Learning to Align and Translate.&quot;</span><br><span class="line">  ICLR 2015. https://arxiv.org/abs/1409.0473</span><br><span class="line"></span><br><span class="line">  The second is the normalized form.  This form is inspired by the</span><br><span class="line">  weight normalization article:</span><br><span class="line"></span><br><span class="line">  Tim Salimans, Diederik P. Kingma.</span><br><span class="line">  &quot;Weight Normalization: A Simple Reparameterization to Accelerate</span><br><span class="line">   Training of Deep Neural Networks.&quot;</span><br><span class="line">  https://arxiv.org/abs/1602.07868</span><br><span class="line"></span><br><span class="line">  To enable the second form, construct the object with parameter</span><br><span class="line">  `normalize=True`.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">  def __init__(self,</span><br><span class="line">               num_units,</span><br><span class="line">               memory,</span><br><span class="line">               memory_sequence_length=None,</span><br><span class="line">               normalize=False,</span><br><span class="line">               probability_fn=None,</span><br><span class="line">               score_mask_value=None,</span><br><span class="line">               dtype=None,</span><br><span class="line">               name=&quot;BahdanauAttention&quot;):</span><br><span class="line">    &quot;&quot;&quot;Construct the Attention mechanism.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      num_units: The depth of the query mechanism.</span><br><span class="line">      memory: The memory to query; usually the output of an RNN encoder.  This</span><br><span class="line">        tensor should be shaped `[batch_size, max_time, ...]`.</span><br><span class="line">      memory_sequence_length (optional): Sequence lengths for the batch entries</span><br><span class="line">        in memory.  If provided, the memory tensor rows are masked with zeros</span><br><span class="line">        for values past the respective sequence lengths.</span><br><span class="line">      normalize: Python boolean.  Whether to normalize the energy term.</span><br><span class="line">      probability_fn: (optional) A `callable`.  Converts the score to</span><br><span class="line">        probabilities.  The default is `tf.nn.softmax`. Other options include</span><br><span class="line">        `tf.contrib.seq2seq.hardmax` and `tf.contrib.sparsemax.sparsemax`.</span><br><span class="line">        Its signature should be: `probabilities = probability_fn(score)`.</span><br><span class="line">      score_mask_value: (optional): The mask value for score before passing into</span><br><span class="line">        `probability_fn`. The default is -inf. Only used if</span><br><span class="line">        `memory_sequence_length` is not None.</span><br><span class="line">      dtype: The data type for the query and memory layers of the attention</span><br><span class="line">        mechanism.</span><br><span class="line">      name: Name to use when creating ops.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if probability_fn is None:</span><br><span class="line">      probability_fn = nn_ops.softmax</span><br><span class="line">    if dtype is None:</span><br><span class="line">      dtype = dtypes.float32</span><br><span class="line">    wrapped_probability_fn = lambda score, _: probability_fn(score)</span><br><span class="line">    super(BahdanauAttention, self).__init__(</span><br><span class="line">        query_layer=layers_core.Dense(</span><br><span class="line">            num_units, name=&quot;query_layer&quot;, use_bias=False, dtype=dtype),</span><br><span class="line">        memory_layer=layers_core.Dense(</span><br><span class="line">            num_units, name=&quot;memory_layer&quot;, use_bias=False, dtype=dtype),</span><br><span class="line">        memory=memory,</span><br><span class="line">        probability_fn=wrapped_probability_fn,</span><br><span class="line">        memory_sequence_length=memory_sequence_length,</span><br><span class="line">        score_mask_value=score_mask_value,</span><br><span class="line">        name=name)</span><br><span class="line">    self._num_units = num_units</span><br><span class="line">    self._normalize = normalize</span><br><span class="line">    self._name = name</span><br><span class="line"></span><br><span class="line">  def __call__(self, query, state):</span><br><span class="line">    &quot;&quot;&quot;Score the query based on the keys and values.</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      query: Tensor of dtype matching `self.values` and shape</span><br><span class="line">        `[batch_size, query_depth]`.</span><br><span class="line">      state: Tensor of dtype matching `self.values` and shape</span><br><span class="line">        `[batch_size, alignments_size]`</span><br><span class="line">        (`alignments_size` is memory&apos;s `max_time`).</span><br><span class="line">        pre step的输出，即score经过softmax，参考next_state</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      alignments: Tensor of dtype matching `self.values` and shape</span><br><span class="line">        `[batch_size, alignments_size]` (`alignments_size` is memory&apos;s</span><br><span class="line">        `max_time`).</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    with variable_scope.variable_scope(None, &quot;bahdanau_attention&quot;, [query]):</span><br><span class="line">      processed_query = self.query_layer(query) if self.query_layer else query</span><br><span class="line">      score = _bahdanau_score(processed_query, self._keys, self._normalize)</span><br><span class="line">    alignments = self._probability_fn(score, state)</span><br><span class="line">    next_state = alignments</span><br><span class="line">    return alignments, next_state</span><br></pre></td></tr></table></figure>
<h2 id="AttentionWrapper"><a href="#AttentionWrapper" class="headerlink" title="AttentionWrapper"></a>AttentionWrapper</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br></pre></td><td class="code"><pre><span class="line">def _compute_attention(attention_mechanism, cell_output, attention_state,</span><br><span class="line">                       attention_layer):</span><br><span class="line">  &quot;&quot;&quot;Computes the attention and alignments for a given attention_mechanism.&quot;&quot;&quot;</span><br><span class="line">  alignments, next_attention_state = attention_mechanism(</span><br><span class="line">      cell_output, state=attention_state)</span><br><span class="line"></span><br><span class="line">  # Reshape from [batch_size, memory_time] to [batch_size, 1, memory_time]</span><br><span class="line">  expanded_alignments = array_ops.expand_dims(alignments, 1)</span><br><span class="line">  # Context is the inner product of alignments and values along the</span><br><span class="line">  # memory time dimension.</span><br><span class="line">  # alignments shape is</span><br><span class="line">  #   [batch_size, 1, memory_time]</span><br><span class="line">  # attention_mechanism.values shape is</span><br><span class="line">  #   [batch_size, memory_time, memory_size]</span><br><span class="line">  # the batched matmul is over memory_time, so the output shape is</span><br><span class="line">  #   [batch_size, 1, memory_size].</span><br><span class="line">  # we then squeeze out the singleton dim.</span><br><span class="line">  context = math_ops.matmul(expanded_alignments, attention_mechanism.values)</span><br><span class="line">  context = array_ops.squeeze(context, [1])</span><br><span class="line"></span><br><span class="line">  if attention_layer is not None:</span><br><span class="line">    attention = attention_layer(array_ops.concat([cell_output, context], 1))</span><br><span class="line">  else:</span><br><span class="line">    attention = context</span><br><span class="line"></span><br><span class="line">  return attention, alignments, next_attention_state</span><br><span class="line"></span><br><span class="line">class AttentionWrapperState(</span><br><span class="line">    collections.namedtuple(&quot;AttentionWrapperState&quot;,</span><br><span class="line">                           (&quot;cell_state&quot;, &quot;attention&quot;, &quot;time&quot;, &quot;alignments&quot;,</span><br><span class="line">                            &quot;alignment_history&quot;, &quot;attention_state&quot;))):</span><br><span class="line">  &quot;&quot;&quot;`namedtuple` storing the state of a `AttentionWrapper`.</span><br><span class="line"></span><br><span class="line">  Contains:</span><br><span class="line"></span><br><span class="line">    - `cell_state`: The state of the wrapped `RNNCell` at the previous time</span><br><span class="line">      step.</span><br><span class="line">    - `attention`: The attention emitted at the previous time step.</span><br><span class="line">    - `time`: int32 scalar containing the current time step.</span><br><span class="line">    - `alignments`: A single or tuple of `Tensor`(s) containing the alignments</span><br><span class="line">       emitted at the previous time step for each attention mechanism.</span><br><span class="line">    - `alignment_history`: (if enabled) a single or tuple of `TensorArray`(s)</span><br><span class="line">       containing alignment matrices from all time steps for each attention</span><br><span class="line">       mechanism. Call `stack()` on each to convert to a `Tensor`.</span><br><span class="line">    - `attention_state`: A single or tuple of nested objects</span><br><span class="line">       containing attention mechanism state for each attention mechanism.</span><br><span class="line">       The objects may contain Tensors or TensorArrays.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">class AttentionWrapper(rnn_cell_impl.RNNCell):</span><br><span class="line">  &quot;&quot;&quot;Wraps another `RNNCell` with attention.</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">  def __init__(self,</span><br><span class="line">               cell,</span><br><span class="line">               attention_mechanism,</span><br><span class="line">               attention_layer_size=None,</span><br><span class="line">               alignment_history=False,</span><br><span class="line">               cell_input_fn=None,</span><br><span class="line">               output_attention=True,</span><br><span class="line">               initial_cell_state=None,</span><br><span class="line">               name=None,</span><br><span class="line">               attention_layer=None):</span><br><span class="line">    &quot;&quot;&quot;Construct the `AttentionWrapper`.</span><br><span class="line">    Args:</span><br><span class="line">      cell: An instance of `RNNCell`.</span><br><span class="line">      attention_mechanism: A list of `AttentionMechanism` instances or a single</span><br><span class="line">        instance.</span><br><span class="line">      attention_layer_size: A list of Python integers or a single Python</span><br><span class="line">        integer, the depth of the attention (output) layer(s). If None</span><br><span class="line">        (default), use the context as attention at each time step. Otherwise,</span><br><span class="line">        feed the context and cell output into the attention layer to generate</span><br><span class="line">        attention at each time step. If attention_mechanism is a list,</span><br><span class="line">        attention_layer_size must be a list of the same length. If</span><br><span class="line">        attention_layer is set, this must be None.</span><br><span class="line">      alignment_history: Python boolean, whether to store alignment history</span><br><span class="line">        from all time steps in the final output state (currently stored as a</span><br><span class="line">        time major `TensorArray` on which you must call `stack()`).</span><br><span class="line">      cell_input_fn: (optional) A `callable`.  The default is:</span><br><span class="line">        `lambda inputs, attention: array_ops.concat([inputs, attention], -1)`.</span><br><span class="line">      output_attention: Python bool.  If `True` (default), the output at each</span><br><span class="line">        time step is the attention value.  This is the behavior of Luong-style</span><br><span class="line">        attention mechanisms.  If `False`, the output at each time step is</span><br><span class="line">        the output of `cell`.  This is the behavior of Bhadanau-style</span><br><span class="line">        attention mechanisms.  In both cases, the `attention` tensor is</span><br><span class="line">        propagated to the next time step via the state and is used there.</span><br><span class="line">        This flag only controls whether the attention mechanism is propagated</span><br><span class="line">        up to the next cell in an RNN stack or to the top RNN output.</span><br><span class="line">      initial_cell_state: The initial state value to use for the cell when</span><br><span class="line">        the user calls `zero_state()`.  Note that if this value is provided</span><br><span class="line">        now, and the user uses a `batch_size` argument of `zero_state` which</span><br><span class="line">        does not match the batch size of `initial_cell_state`, proper</span><br><span class="line">        behavior is not guaranteed.</span><br><span class="line">      name: Name to use when creating ops.</span><br><span class="line">      attention_layer: A list of `tf.layers.Layer` instances or a</span><br><span class="line">        single `tf.layers.Layer` instance taking the context and cell output as</span><br><span class="line">        inputs to generate attention at each time step. If None (default), use</span><br><span class="line">        the context as attention at each time step. If attention_mechanism is a</span><br><span class="line">        list, attention_layer must be a list of the same length. If</span><br><span class="line">        attention_layers_size is set, this must be None.</span><br><span class="line">    Raises:</span><br><span class="line">      TypeError: `attention_layer_size` is not None and (`attention_mechanism`</span><br><span class="line">        is a list but `attention_layer_size` is not; or vice versa).</span><br><span class="line">      ValueError: if `attention_layer_size` is not None, `attention_mechanism`</span><br><span class="line">        is a list, and its length does not match that of `attention_layer_size`;</span><br><span class="line">        if `attention_layer_size` and `attention_layer` are set simultaneously.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"> ......</span><br><span class="line"></span><br><span class="line">  def call(self, inputs, state):</span><br><span class="line">    &quot;&quot;&quot;Perform a step of attention-wrapped RNN.</span><br><span class="line"></span><br><span class="line">    - Step 1: Mix the `inputs` and previous step&apos;s `attention` output via</span><br><span class="line">      `cell_input_fn`.</span><br><span class="line">    - Step 2: Call the wrapped `cell` with this input and its previous state.</span><br><span class="line">    - Step 3: Score the cell&apos;s output with `attention_mechanism`.</span><br><span class="line">    - Step 4: Calculate the alignments by passing the score through the</span><br><span class="line">      `normalizer`.</span><br><span class="line">    - Step 5: Calculate the context vector as the inner product between the</span><br><span class="line">      alignments and the attention_mechanism&apos;s values (memory).</span><br><span class="line">    - Step 6: Calculate the attention output by concatenating the cell output</span><br><span class="line">      and context through the attention layer (a linear layer with</span><br><span class="line">      `attention_layer_size` outputs).</span><br><span class="line"></span><br><span class="line">    Args:</span><br><span class="line">      inputs: (Possibly nested tuple of) Tensor, the input at this time step.</span><br><span class="line">      state: An instance of `AttentionWrapperState` containing</span><br><span class="line">        tensors from the previous time step.</span><br><span class="line"></span><br><span class="line">    Returns:</span><br><span class="line">      A tuple `(attention_or_cell_output, next_state)`, where:</span><br><span class="line"></span><br><span class="line">      - `attention_or_cell_output` depending on `output_attention`.</span><br><span class="line">      - `next_state` is an instance of `AttentionWrapperState`</span><br><span class="line">         containing the state calculated at this time step.</span><br><span class="line"></span><br><span class="line">    Raises:</span><br><span class="line">      TypeError: If `state` is not an instance of `AttentionWrapperState`.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    if not isinstance(state, AttentionWrapperState):</span><br><span class="line">      raise TypeError(&quot;Expected state to be instance of AttentionWrapperState. &quot;</span><br><span class="line">                      &quot;Received type %s instead.&quot;  % type(state))</span><br><span class="line">    # Step 1: Calculate the true inputs to the cell based on the</span><br><span class="line">    # previous attention value.</span><br><span class="line">    cell_inputs = self._cell_input_fn(inputs, state.attention)</span><br><span class="line">    cell_state = state.cell_state</span><br><span class="line">    cell_output, next_cell_state = self._cell(cell_inputs, cell_state)</span><br><span class="line"></span><br><span class="line">    cell_batch_size = (</span><br><span class="line">        cell_output.shape[0].value or array_ops.shape(cell_output)[0])</span><br><span class="line">    error_message = (</span><br><span class="line">        &quot;When applying AttentionWrapper %s: &quot; % self.name +</span><br><span class="line">        &quot;Non-matching batch sizes between the memory &quot;</span><br><span class="line">        &quot;(encoder output) and the query (decoder output).  Are you using &quot;</span><br><span class="line">        &quot;the BeamSearchDecoder?  You may need to tile your memory input via &quot;</span><br><span class="line">        &quot;the tf.contrib.seq2seq.tile_batch function with argument &quot;</span><br><span class="line">        &quot;multiple=beam_width.&quot;)</span><br><span class="line">    with ops.control_dependencies(</span><br><span class="line">        self._batch_size_checks(cell_batch_size, error_message)):</span><br><span class="line">      cell_output = array_ops.identity(</span><br><span class="line">          cell_output, name=&quot;checked_cell_output&quot;)</span><br><span class="line"></span><br><span class="line">    if self._is_multi:</span><br><span class="line">      previous_attention_state = state.attention_state</span><br><span class="line">      previous_alignment_history = state.alignment_history</span><br><span class="line">    else:</span><br><span class="line">      previous_attention_state = [state.attention_state]</span><br><span class="line">      previous_alignment_history = [state.alignment_history]</span><br><span class="line">    all_alignments = []</span><br><span class="line">    all_attentions = []</span><br><span class="line">    all_attention_states = []</span><br><span class="line">    maybe_all_histories = []</span><br><span class="line">    for i, attention_mechanism in enumerate(self._attention_mechanisms):</span><br><span class="line">      attention, alignments, next_attention_state = _compute_attention(</span><br><span class="line">          attention_mechanism, cell_output, previous_attention_state[i],</span><br><span class="line">          self._attention_layers[i] if self._attention_layers else None)</span><br><span class="line">      alignment_history = previous_alignment_history[i].write(</span><br><span class="line">          state.time, alignments) if self._alignment_history else ()</span><br><span class="line"></span><br><span class="line">      all_attention_states.append(next_attention_state)</span><br><span class="line">      all_alignments.append(alignments)</span><br><span class="line">      all_attentions.append(attention)</span><br><span class="line">      maybe_all_histories.append(alignment_history)</span><br><span class="line"></span><br><span class="line">    attention = array_ops.concat(all_attentions, 1)</span><br><span class="line">    next_state = AttentionWrapperState(</span><br><span class="line">        time=state.time + 1,</span><br><span class="line">        cell_state=next_cell_state,</span><br><span class="line">        attention=attention,</span><br><span class="line">        attention_state=self._item_or_tuple(all_attention_states),</span><br><span class="line">        alignments=self._item_or_tuple(all_alignments),</span><br><span class="line">        alignment_history=self._item_or_tuple(maybe_all_histories))</span><br><span class="line"></span><br><span class="line">    if self._output_attention:</span><br><span class="line">      return attention, next_state</span><br><span class="line">    else:</span><br><span class="line">      return cell_output, next_state</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2018/12/05/gcc简介/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/05/gcc简介/" itemprop="url">
                  gcc简介
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-12-05 11:54:04 / 修改时间：12:02:22" itemprop="dateCreated datePublished" datetime="2018-12-05T11:54:04+08:00">2018-12-05</time>
            

            
              

              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/12/05/gcc简介/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/12/05/gcc简介/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>gcc即GNU编译器套件（GNU Compiler Collection）包括C、C++、Objective-C、Fortran、Java、Ada和Go语言的前端，也包括了这些语言的库（如libstdc++、libgcj等等）<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/12/05/gcc简介/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2018/10/25/Mac下使用pyenv管理Python版本/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/25/Mac下使用pyenv管理Python版本/" itemprop="url">
                  Mac下使用pyenv管理Python版本
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-25 23:48:18" itemprop="dateCreated datePublished" datetime="2018-10-25T23:48:18+08:00">2018-10-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-10-26 00:36:55" itemprop="dateModified" datetime="2018-10-26T00:36:55+08:00">2018-10-26</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/25/Mac下使用pyenv管理Python版本/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/10/25/Mac下使用pyenv管理Python版本/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Mac OS自带的Python版本是2.x，pip安装tensorflow到系统自带Python会失败，又不想使用anaconda，所以希望自己安装一个非系统的Python，同时有多个版本，方便使用，pyenv就是这样一个Python版本管理器。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/10/25/Mac下使用pyenv管理Python版本/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2018/10/24/优化MAC系统设置/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/24/优化MAC系统设置/" itemprop="url">
                  优化MAC系统设置
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-24 01:07:58 / 修改时间：01:13:23" itemprop="dateCreated datePublished" datetime="2018-10-24T01:07:58+08:00">2018-10-24</time>
            

            
              

              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/24/优化MAC系统设置/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/10/24/优化MAC系统设置/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>MAC系统的设置提供了丰富的自定义功能，拿到mac后第一件事就是优化系统设置<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/10/24/优化MAC系统设置/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2018/10/21/vi编辑器使用教程/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/21/vi编辑器使用教程/" itemprop="url">
                  vi编辑器使用教程
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-21 14:30:46" itemprop="dateCreated datePublished" datetime="2018-10-21T14:30:46+08:00">2018-10-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-10-23 23:34:54" itemprop="dateModified" datetime="2018-10-23T23:34:54+08:00">2018-10-23</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/21/vi编辑器使用教程/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/10/21/vi编辑器使用教程/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>一开始用私人服务器，linux系统都是ubuntu，有Xwindow，所以编辑文件是直接用的gedit，把linux活生生用成了window，因为最近需要使用ssh远程，只有tty，改用vi，指令不熟练，稍微记录一下基本指令。<br>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/10/21/vi编辑器使用教程/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2018/10/20/MarkDown基本语法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/20/MarkDown基本语法/" itemprop="url">
                  MarkDown基本语法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-20 14:28:14" itemprop="dateCreated datePublished" datetime="2018-10-20T14:28:14+08:00">2018-10-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2018-10-23 23:34:54" itemprop="dateModified" datetime="2018-10-23T23:34:54+08:00">2018-10-23</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/20/MarkDown基本语法/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/10/20/MarkDown基本语法/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>因为github发表博客需要使用MarkDown，因此了解一下基本语法，MarkDown的基础语法比较简单，5min就可以掌握。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/10/20/MarkDown基本语法/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://www.juntengzhang.com/2018/10/18/hello-world/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="张君腾">
      <meta itemprop="description" content="Talk is cheap, show me the code!">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="张君腾的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/10/18/hello-world/" itemprop="url">
                  Hello World
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2018-10-18 12:30:00" itemprop="dateCreated datePublished" datetime="2018-10-18T12:30:00+08:00">2018-10-18</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">更新于</span>
                
                <time title="修改时间：2019-04-24 17:25:47" itemprop="dateModified" datetime="2019-04-24T17:25:47+08:00">2019-04-24</time>
              
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/10/18/hello-world/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2018/10/18/hello-world/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2018/10/18/hello-world/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">张君腾</p>
              <p class="site-description motion-element" itemprop="description">Talk is cheap, show me the code!</p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">2</span>
                    <span class="site-state-item-name">标签</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  <a href="https://github.com/juntengzhang" target="_blank" title="GitHub"><i class="fa fa-fw fa-github"></i>GitHub</a>
                  
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:zjt.aig@gmail.com" target="_blank" title="E-Mail"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  
                </span>
              
            </div>
          

          
          

          
          

          
            
          
          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-fas fa-heartbeat"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">张君腾</span>

  

  
</div>











        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv" title="总访客量">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  

  
    <span class="site-pv" title="总访问量">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>









        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    
	
    

    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  













  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.4.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.4.2"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.4.2"></script>



  



  








  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  
  
  <script src="//unpkg.com/valine/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(function (item) {
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: false,
        appId: '9TfGfCHzKGeIDsmICfXkCAoV-gzGzoHsz',
        appKey: 'rAtUIg89jE4kUUCSgG4NCX2o',
        placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
        avatar:'mm',
        meta:guest,
        pageSize:'10' || 10,
        visitor: false
    });
  </script>



  





  

  

  

  

  
  

  

  

  

  

  

<script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/z16.model.json"},"display":{"position":"right","width":100,"height":250},"mobile":{"show":true},"log":false});</script></body>
</html>
